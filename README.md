# seq2seq-lstm-translator
*   In this project, we will walk through how to build an LSTM-based Neural Translation (Sequence-to-Sequence) model from scratch.
*   This project is beginner-friendly but basic understanding in NN and NLP will be assumed.
*   We will cover topics like Text Preprocessing, Padding, Masking, Embedding, LSTM (you can substitue with RNN or GRU), Encoder / Decoder Architecture, Perplexity, and more.
*   I will use TensorFlow's functional API to create standard model structures (you can use Sequential if it is preferred). And for custom layers and training loops, the Subclassing API will be used.
*   I have kept everything on a Jupyter Notebook format in hopes to provide a clearer workflow and step-by-step explanations. A PDF version will also be provided in case you don't want to execute the codes.
*   I am a graduate student at McMaster University and this is my first post on Github. If you have any suggestion for my code, or questions, please feel free to reach out to me on LinkedIn: https://www.linkedin.com/in/saberyu/. 











